{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6pm0CMVPJmM"
   },
   "source": [
    "---\n",
    "## __Question 1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJPQRbtvPJmO"
   },
   "source": [
    "### __Introduction__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnWa00IXPJmP"
   },
   "source": [
    "- #### The problem consists of calculating the frequency of each alphabet in the file and then calculating their probability, uncertainty and entropy.\n",
    "- #### Primarily, the data structure of dictionary is used for storage and the prettytable library is used to print in a better format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZPevOC2PJmQ"
   },
   "source": [
    "### __Procedure__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hHB92BUPJmQ"
   },
   "source": [
    "#### The method used here is simple and it consists of the following mentioned steps.\n",
    "  1. ##### First, I read from the file line-by-line to avoid all the data. I end this step by checking whether the file has ended/not. During this, each letter is converted into lowercase to avoid confusion.\n",
    "  2. ##### Afterward, I have a dictionary which has the _alphabets_ as _Keys_ and their _frequency_ in the file as _Values_.\n",
    "  3. ##### I normalize this dictionary and create a new dictionary which has _Probability_ as the values to the keys.\n",
    "  4. ##### With the help of the function given below, I applied it to get the individual uncertainty and store it in another dictionary.\n",
    "$$ U(i) =  (-p_i\\;log\\;p_i) $$  \n",
    "  5. ##### Finally, we sum all the Uncertainty values to get the _Entropy_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyFuvNxOPJmR"
   },
   "source": [
    "### __Imports and Functions used__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2d3vX6aPJmR"
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "from math import log2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pretty_table( message:str , obj:dict )->None:\n",
    "  \"\"\"Summary\n",
    "      This prints the dictionary in a better format with the help of prettytable library.\n",
    "\n",
    "  Args:\n",
    "      message (str): This is the message which is to be passed alongside the dictionary.\n",
    "      obj (dict): This is the dictionary which is to be inputted to be printed in the prettytable format.\n",
    "  \"\"\"\n",
    "\n",
    "  TABLE = PrettyTable()\n",
    "  for c in obj.keys():\n",
    "    TABLE.add_column(c, [  ])\n",
    "  TABLE.add_row( [ obj[ ky ] for ky in obj.keys() ] )\n",
    "\n",
    "  print( f'{message}' , TABLE )\n",
    "\n",
    "\n",
    "def file_letter( path:str )->dict:\n",
    "  \"\"\"Summary\n",
    "\n",
    "  Args:\n",
    "      path (str): This is the variable which takes in the path for the file from which data is to be checked.\n",
    "\n",
    "  Returns:\n",
    "      dict: It returns a dict which has letter as keys and their frequencies as the value to the key.\n",
    "  \"\"\"\n",
    "  alphabet_count : dict = {}\n",
    "\n",
    "  # Making the dictionary of the format mentioned above\n",
    "  for i in range(97,123,1):\n",
    "    alphabet_count[ chr(i) ] = 0\n",
    "\n",
    "  sum_ = 0\n",
    "  # Opening the file and processing count line by line to avoid storing overheads\n",
    "  with open( path , 'r') as file:\n",
    "\n",
    "    while True:\n",
    "      line:str = file.readline()\n",
    "\n",
    "      for letter in line:\n",
    "\n",
    "        temp = alphabet_count.get( letter.lower() )\n",
    "\n",
    "        if temp!=None:\n",
    "          alphabet_count[ letter.lower() ] += 1\n",
    "          sum_ += 1\n",
    "\n",
    "      # Checking for the end of file\n",
    "      if \"\" == line :\n",
    "        break\n",
    "\n",
    "  return alphabet_count\n",
    "\n",
    "\n",
    "def entropy( Uncertainty:dict )->float:\n",
    "  \"\"\"Summary:\n",
    "    It returns the Entropy of the values which were passed in the dictionary as the keys.\n",
    "\n",
    "  Args:\n",
    "      Uncertainty (dict): This is a dictionary which contains the alphabets as keys and their uncertainty in the file as the values .\n",
    "  \"\"\"\n",
    "\n",
    "  Entropy =0\n",
    "  for key in Uncertainty:\n",
    "    Entropy += Uncertainty[ key ]\n",
    "  return Entropy\n",
    "\n",
    "def top_10( alpha_frequency:dict )->dict:\n",
    "  \"\"\"Summary\n",
    "        This returns a dictionary of the top 10 alphabets and their frequencies sorted by their frequency\n",
    "  Args:\n",
    "      alpha_frequency (dict): Alphabets are the keys and their frequency from the file are the values.\n",
    "\n",
    "  Returns:\n",
    "      dict: Dictionary with top 10 alphabets\n",
    "  \"\"\"\n",
    "  top = {  i:alpha_frequency[i]  for i in (sorted(alpha_frequency,key=alpha_frequency.get, reverse=True))[:10] }\n",
    "  return top\n",
    "\n",
    "\n",
    "def probab( alpha_frequency:dict )->dict:\n",
    "  \"\"\"Summary\n",
    "      This returns a dictionary where keys are the alphabets and the values are the individual probability of occuring in the file.\n",
    "  Args:\n",
    "      alpha_frequency (dict): _description_\n",
    "\n",
    "  Returns:\n",
    "      dict: _description_\n",
    "  \"\"\"\n",
    "\n",
    "  sum_ = sum(alpha_frequency.values())\n",
    "  alpha_prob = { key:alpha_frequency[key]/sum_ for key in alpha_frequency }\n",
    "\n",
    "  return alpha_prob\n",
    "\n",
    "def uncertain( alpha_probab:dict )->dict:\n",
    "  \"\"\"Summary\n",
    "    This returns a dictionary where the keys are the alphabets and the values are the individual uncertainty of the alphabets.\n",
    "\n",
    "  Args:\n",
    "      alpha_probab (dict): Dictionary with alphabets as keys and their probability of occuring in the file as values.\n",
    "\n",
    "  Returns:\n",
    "      dict: Dictionary with alphabets as keys and their indvidual uncertainty as values.\n",
    "  \"\"\"\n",
    "  Uncertainty = { key : ( -1 * alpha_probab[ key ] *  log2( alpha_probab[ key ] ) ) for key in alpha_probab }\n",
    "  return Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCd4gPzMPJmS"
   },
   "source": [
    "### __Code__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtS8Qo79PJmT"
   },
   "source": [
    "#### __File A: Time Machine__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wM_i3hw2PJmU",
    "outputId": "84848ad0-52fd-4f92-caf8-441885988c00"
   },
   "outputs": [],
   "source": [
    "# The frequency of each letter\n",
    "fileA_count = file_letter( './fileA.txt' )\n",
    "\n",
    "# Top 10 most occuring element\n",
    "fileA_top10 = top_10( fileA_count )\n",
    "pretty_table( '\\nTop 10 most occuring element:\\n',top_10( fileA_count ) )\n",
    "\n",
    "# The probability for each alphabet\n",
    "fileA_probab = probab( fileA_count )\n",
    "pretty_table( '\\nProbabiltity for each element:\\n' , fileA_probab )\n",
    "\n",
    "# The Uncertainty for each alphabet\n",
    "fileA_uncertain = uncertain( fileA_probab )\n",
    "pretty_table( '\\nThe Uncertainlty for each element:\\n' , fileA_uncertain )\n",
    "\n",
    "# The Total Entropy\n",
    "fileA_entropy = entropy( fileA_uncertain )\n",
    "print( f\"\\nThe total Entropy is {fileA_entropy}\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJuR3eUnPJmU"
   },
   "source": [
    "#### __File B: Jane Eyre__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scSA5Yi0PJmU",
    "outputId": "b3540b95-cbe7-499c-ed34-b2b72421ef41"
   },
   "outputs": [],
   "source": [
    "# The frequency of each letter\n",
    "fileB_count = file_letter( './fileB.txt' )\n",
    "\n",
    "# Top 10 most occuring element\n",
    "fileB_top10 = top_10( fileB_count )\n",
    "pretty_table( '\\nTop 10 most occuring element:\\n',top_10( fileB_count ) )\n",
    "\n",
    "# The probability for each alphabet\n",
    "fileB_probab = probab( fileB_count )\n",
    "pretty_table( '\\nProbabiltity for each element:\\n' , fileB_probab )\n",
    "\n",
    "# The Uncertainty for each alphabet\n",
    "fileB_uncertain = uncertain( fileB_probab )\n",
    "pretty_table( '\\nThe Uncertainlty for each element:\\n' , fileB_uncertain )\n",
    "\n",
    "# The Total Entropy\n",
    "fileB_entropy = entropy( fileB_uncertain )\n",
    "print( f\"\\nThe total Entropy is {fileB_entropy}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tseTo4ZuPJmV"
   },
   "source": [
    "### __Plots__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "pZXSTeHBPJmV",
    "outputId": "816981e8-2fd0-47ad-abd6-185cfe008fc7"
   },
   "outputs": [],
   "source": [
    "# Making the condition for the plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.figure( figsize=(16,8) )\n",
    "\n",
    "# Plotting for file A\n",
    "plt.subplot(1,2,1)\n",
    "plt.xlabel('Transformed States');   plt.ylabel('Probability')\n",
    "plt.title('Top 10 most occuring alphabets in File A')\n",
    "plt.bar(fileA_top10.keys(), fileA_top10.values(), edgecolor=\"white\",linewidth=1 )\n",
    "\n",
    "# Plotting for file B\n",
    "plt.subplot(1,2,2)\n",
    "plt.xlabel('Transformed States');   plt.ylabel('Probability')\n",
    "plt.title('Top 10 most occuring alphabets in File B')\n",
    "plt.bar(fileB_top10.keys(), fileB_top10.values(), edgecolor=\"white\",linewidth=1)\n",
    "\n",
    "# Showing the bar plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEfIQ82oPJmV"
   },
   "source": [
    "\n",
    "### __Observations__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WtDHul5PJmV"
   },
   "source": [
    "#### File A\n",
    "1. The Top 10 most occuring element and their frequencies are as follows:\n",
    "   \n",
    "   \n",
    "\n",
    "|  e  |  t  |  a  |  i  |  o  |  n  |  s  |  r  |  h  |  d  |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 19678 | 15072 | 12717 | 11262 | 11108 | 10969 | 9288 | 8855 | 8798 | 6857 |\n",
    "\n",
    "2. The Uncertainty in of the alphabets are as follows:\n",
    "    \n",
    "    \n",
    "|  a  |  b  |  c  |  d  |  e  |  f  |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 0.294737626983606 | 0.08566858314546652 | 0.1362084323586161 | 0.19807035932467246 | 0.3766658274599944 | 0.12901373901384622 |\n",
    "|  g  |  h  |  i  |  j  |  k  |  l  |\n",
    "| 0.12267568177448293 | 0.23386819335137668 | 0.2736638257967753 | 0.011415301416179365 | 0.05478010249529092 | 0.1936561430548301 |\n",
    "|  m  |  n  |  o  |  p  |  q  |  r  |\n",
    "| 0.14526025579082738 | 0.2692166950420098 | 0.2713353283863137 | 0.10572991128226748 | 0.007147143147259167 | 0.2348548045059465 |\n",
    "|  s  |  t  |  u  |  v  |  w  |  x  |\n",
    "| 0.24224038198928632 | 0.325649623910887 | 0.1429721386434424 | 0.06178766359885385 | 0.12314104537709523 | 0.016020070806682458 |\n",
    "|  y  |  z  |\n",
    "| 0.10996196523156698 | 0.007263335338108487 |\n",
    "  \n",
    "3. The Entropy for the English alphabets is _4.173_.\n",
    "\n",
    "4. If all the alphabets were equiprobable, then the entropy would approximately be _-4.70_.\n",
    "---\n",
    "#### File B\n",
    "\n",
    "1. The Top 10 most occuring element and their frequencies are as follows:\n",
    "      \n",
    "      \n",
    "  |  e  |  t  |  a  |  o  |  i  |  n  |  s  |  r  |  h  |  d  |\n",
    "  | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "  | 102429 | 68576 | 63844 | 61867 | 57146 | 55278 | 50845 | 48538 | 46393 | 37972 |\n",
    "\n",
    "2. The Uncertainty in of the alphabets is as follows:\n",
    "  \n",
    "  \n",
    "  |  a  |  b  |  c  |  d  |  e  |  f  |\n",
    "  | --- | --- | --- | --- | --- | --- |\n",
    "  | 0.2904216689629503 | 0.08726700028685597 | 0.12951754741050822 | 0.2081795671705995 | 0.37894691087836807 | 0.11907714898815029 |\n",
    "  |  g  |  h  |  i  |  j  |  k  |  l  |\n",
    "  | 0.10996172959736045 | 0.23765199869099976 | 0.27133229441257173 | 0.015222875890138082 | 0.053893841416423276 | 0.18903276586359158 |\n",
    "  |  m  |  n  |  o  |  p  |  q  |  r  |\n",
    "  | 0.14513897349056004 | 0.26576359328020754 | 0.28492482736885494 | 0.09405732455781027 | 0.011575417387160464 | 0.24469839713651606 |\n",
    "  |  s  |  t  |  u  |  v  |  w  |  x  |\n",
    "  | 0.2520870290870917 | 0.30313792977601334 | 0.1513076183939417 | 0.06452473862072222 | 0.1276745564227206 | 0.015057465959174107 |\n",
    "  |  y  |  z  |\n",
    "  | 0.12088704618803803 | 0.004634990065432768 |\n",
    "\n",
    "3. The Entropy for the English alphabets is 4.176.\n",
    "\n",
    "4. If all the alphabets were equiprobable, then the entropy would approximately be _-4.70_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSrao2IUPJmV"
   },
   "source": [
    "### __Result__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ll3UVnJWPJmW"
   },
   "source": [
    "- #### _Common word_ : We can see that the 10 most occuring letter in the two files are nearly same. Furthermore, The most common letter is e, which also coincides with the fact that e is used the most times in English language itself.\n",
    "- #### Similar _Uncertainty_ and _Entropy_: The value of uncertainty as calculated for the english alphabets in the two files is nearly same. This indicates that for very large text data, the distribution of alphabets follows a near similar pattern.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e355SKhuPJmW"
   },
   "source": [
    "---\n",
    "## __Question 2__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYKfSyJgPJmW"
   },
   "source": [
    "### __Introduction__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXWIbYjEPJmW"
   },
   "source": [
    "- #### In the question, I am provided with 16 different states and their respective probability values.\n",
    "- #### Since for a discrete varible, it is hard to get a proper uniform distribution. But we can get a near uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzlra0IiPJmW"
   },
   "source": [
    "### __Procedure__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bBbNoQ1PJmX"
   },
   "source": [
    "#### The procedure followed is given as follows:\n",
    "1. #### First, I have normalized the variable initially given to obtain the pdf of the discrete variable.\n",
    "2. #### After that, we obtained the cdf for the variable.\n",
    "3. #### We use this cdf to transform this discrete variable.\n",
    "4. #### After that, we plot the distribution of the new variable with the help of the bar plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2Ju-leBPJmX"
   },
   "source": [
    "### __Imports and Functions Used__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGePPBuxPJmX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "def Transformation_cdf( cdf:list )->list:\n",
    "  \"\"\"\n",
    "  This function is the transformation function. Here the cdf is used to transform the discrete variable.\n",
    "\n",
    "  Args:\n",
    "      cdf (list): This is a list which cotains the value of the cdf of the discrete variable.\n",
    "\n",
    "  Returns:\n",
    "      ans (list):\n",
    "  \"\"\"\n",
    "  n = len( cdf ); ans = []; temp = 0\n",
    "\n",
    "  for i in range( n ):\n",
    "\n",
    "    ans.append( round( (n-1)*cdf[i] ) )\n",
    "\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyoqFMRZPJmX"
   },
   "source": [
    "### __Code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LsvxtmWPJmX"
   },
   "outputs": [],
   "source": [
    "# ---------------------PART 1-----------------------\n",
    "X = [\n",
    "  ('r0' ,  0.5) , ('r1' , 0.25) , ( 'r2' , 0.125 ) , ( 'r3' ,  0.0625 ) , ('r4' ,  0.03125) , ( 'r5' ,  0.015625 ) , ( 'r6' , 0.03125 ) , ( 'r7' , 0.015625 ), ( 'r8' , 0.0078125 ) , ( 'r9' , 0.00390625 ) , ( 'r10' , 0.001953125 ) , ( 'r11' , 0.00390625 ) , ( 'r12' , 0.001953125 ) , ( 'r13' , 0.0009765625 ) , ( 'r14' , 0.00048828125 ) , ( 'r15' , 0.000244140625 )\n",
    "]\n",
    "# Taking the sum of the probability of the different states which will be later used as a normalizing value.\n",
    "norm = sum( [ elem[1] for elem in X ] )\n",
    "# After normalizing, we get the pdf of X and similarly, we can calculate the cdf of the variable\n",
    "PDF = [  ( elem[0] , elem[1]/norm )  for elem in X  ]\n",
    "CDF = [ ];  temp = 0\n",
    "\n",
    "for i in  range( len( PDF ) ):\n",
    "  temp += PDF[i][1]\n",
    "  CDF.append( temp )\n",
    "\n",
    "# Making the variable Y which is transformed using the cdf of X\n",
    "Prob_Y = [ 0 for i in range(len(CDF))]\n",
    "\n",
    "# Storing the transformation in a new list\n",
    "transform = Transformation_cdf( CDF )\n",
    "for i in range( len(CDF) ):\n",
    "  pos = int( transform[i] )\n",
    "  Prob_Y[pos] +=  X[i][1]\n",
    "Y = [ ( f's{i}' , Prob_Y[i] ) for i in range( len(CDF) )]\n",
    "\n",
    "# Creating the dictionary for the data to be displayed\n",
    "Variable = {\n",
    "  'States' : [ elm[0] for elm in X ], 'Given probability' : [ elm[1] for elm in X ], 'PDF': [ elm[1] for elm in PDF ], 'CDF':CDF\n",
    "}\n",
    "\n",
    "Y_pdf = {\n",
    "  'States' : [ elm[0] for elm in Y ], 'PDF' : [ elm[1] for elm in Y ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "m-Twabf5PJmX",
    "outputId": "b68806aa-896e-4f1f-db93-2344b8717f30"
   },
   "outputs": [],
   "source": [
    "print( \"Initial Variable\" )\n",
    "DataFrame( Variable )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "JnpuKMO9PJmY",
    "outputId": "a8860e88-e379-4a56-d4dc-b929c589c1f4"
   },
   "outputs": [],
   "source": [
    "print( \"Final Variable\" )\n",
    "DataFrame( Y_pdf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJpx_U99PJmY"
   },
   "source": [
    "### __Plot__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "CMiVTBfOPJmY",
    "outputId": "3b25fcb3-de20-494e-d947-c17efbd3b6f6"
   },
   "outputs": [],
   "source": [
    "# Making the condition for the plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.figure( figsize=(7,7) )\n",
    "plt.xlabel('Transformed States');   plt.ylabel('Probability')\n",
    "plt.bar(Y_pdf['States'], Y_pdf['PDF'], edgecolor=\"white\",linewidth=1 )\n",
    "\n",
    "# Showing the bar plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDOmLGdHPJmY"
   },
   "source": [
    "### __Observations__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjPj-1LEPJmY"
   },
   "source": [
    "- #### Intially, the maximum probability(nearly 50%) was of the first state.\n",
    "- #### After the transformation, the probability distribution has shifted with its peak nearly at the middle.\n",
    "- #### A Proper uniform distribution is not obtained here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MtjrrIZPJmZ"
   },
   "source": [
    "### __Result__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4HHgWSGPJmZ"
   },
   "source": [
    "- #### The transformation of a discrete variable has a very less probability to lead to a Uniform Distribution.\n",
    "- #### The reason for the above statement can be subjected to the approximation of the transformation values obtained to get a proper state-to-state transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5sqO0lvPJmZ"
   },
   "source": [
    "---\n",
    "## __Question 3__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5Y_ThysPJmZ"
   },
   "source": [
    "### __Introduction__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSrPHQlLPJmZ"
   },
   "source": [
    "#### In the above question, it is asked to obtain random numbers which have the distribution as shown in the question.\n",
    "#### The distribution shown in the question has the _pdf_ given below:\n",
    "$$\n",
    "  f(x) = \\dfrac{(x-1)}{3} \\;\\;\\;\\;  for \\;\\;1\\leq x\\leq3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ey4uMf6APJma"
   },
   "source": [
    "### __Procedure__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT_RYKnmPJma"
   },
   "source": [
    "#### The procedure we have here is the _inverse transformation sampling_ and the complete procedure used is given as follows\n",
    "  1. #### We know that if we use the _cumulative distribution function_  as the transformation, then we get a Variable which follows _Uniform Distribution_.\n",
    "  $$  F_Y(y) = P(Y\\leq y) $$\n",
    "  $$ F_Y(y) = P( g(X)\\leq y ) \\;\\;\\;\\;  where \\;\\;\\;\\;   g(X) = F_X(x) $$\n",
    "\n",
    "  2. #### Since, g is an increasing function.\n",
    "  $$  F_Y(y) = P(X\\leq g^{-1}(Y))  $$\n",
    "  $$ F_Y(y) = F_X(g^{-1}(Y)) $$\n",
    "  \n",
    "  3. #### We know that the pdf is obtained by differentiating the cdf for a continuous variable.\n",
    "  $$ f(y) = \\dfrac{d}{dx} \\; F_Y(y) $$\n",
    "  $$ f(y) = F_X^{'}(g^{-1}(Y))\\;(g^{-1}(Y))^{'} $$\n",
    "  $$ f(y) = f_X(g^{-1}(y))\\;\\;\\dfrac{d}{dy}\\;g^{-1}(Y) $$\n",
    "  \n",
    "  4. #### After doing the above operations for the function, we will obtain the f(y) as 1. We can normalize it.\n",
    "  $$ y = \\dfrac{(x-1)^2}{4} $$\n",
    "  5. #### After inverting it, we get the function which is to be used for transformation.\n",
    "  $$ x = 1 + \\sqrt{4y}$$\n",
    "  $$ x = 1 + 2\\sqrt{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16Y80j_dPJma"
   },
   "source": [
    "### __Imports and Functions Used__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmbSK11JPJma"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import uniform as rand\n",
    "from numpy import sqrt as nsqrt\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZomc979PJma"
   },
   "source": [
    "### __Code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "9pPgKEG0PJma",
    "outputId": "6fe17e3b-8653-4985-c82b-af8a37110f19"
   },
   "outputs": [],
   "source": [
    "data = { }\n",
    "\n",
    "# Regulating the size of random numbers generated\n",
    "ln = 10000\n",
    "\n",
    "data['Random_num'] = rand(0, 1, ln)\n",
    "data['Prob'] = ( 1 + 2* nsqrt( data['Random_num'] ))\n",
    "DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Plots__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "rl0l0GO8PJma",
    "outputId": "d4bcca27-511f-450e-ae53-f80bbabea340"
   },
   "outputs": [],
   "source": [
    "# Adjusting the bin size to observe the impact on the visualisation\n",
    "bin_size = 1000\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.hist( data['Prob'] , bins=bin_size)\n",
    "plt.title('PDF')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTCLafg8PJmb"
   },
   "source": [
    "### __Observations__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4JqeUocPJmb"
   },
   "source": [
    "- #### Inverse transformation sampling, which we have used above takes samples between 0 and 1, which can be said to be probability, and then returns the smallest number\n",
    "$$ x âˆˆ R \\;\\;such \\;\\; that \\;\\;\\;y \\leq f(x) $$\n",
    "- #### As we increase the bin size further, we will see that the psuedo-random numbers which are generated are from the distribution given in the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWFjkJIIPJmb"
   },
   "source": [
    "### __Results__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sy8viY3YPJmb"
   },
   "source": [
    "- #### This is in line with the fact that if we apply cdf as a transformation, we get a variable which follows random distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBaNx6zTPJmb"
   },
   "source": [
    "---\n",
    "## __Question 4__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsJIN0qGPJmc"
   },
   "source": [
    "### __Introduction__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_ucBaNFPJmc"
   },
   "source": [
    "- #### Here, A gray-scale image is to be taken and its intensity histogram is to be plotted. After that, we have to build a equalisation function to get a equalised histogram.\n",
    "- #### After that, a random value transformation to the gray value is to be applied to get a uniform PMF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHbOxOwIPJmc"
   },
   "source": [
    "### __Procedure__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vZy2QITPJmc"
   },
   "source": [
    "#### The procedure followed is given as follows:\n",
    "  1. #### With the help of OpenCV, the image is read. Since the image taken initially is colored, it is converted to the grayscale form.\n",
    "  2. #### Next, I generated the image histogram with the help of a list with of size 256 where the index value is the intensity.\n",
    "  3. #### After that the histogram is visulaized using different value of bins.\n",
    "  4. #### Then I created the `Image_equilize` function which equalises the image with the help of transformation of discrete varible to a variable following a near _Uniform Distribution_. This is done by using the cdf as the transformation function.\n",
    "  5. #### Afterwards, we see the difference in the image and histogram of the initial grayscale image and the equalised one.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDu9c09qPJmd"
   },
   "source": [
    "### __Imports and Functions used__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5ieFKi2PJmd"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def pretty_table( message , obj ):\n",
    "  \"\"\"It just prints the dictionary containing letter count in a better format. \"\"\"\n",
    "\n",
    "  TABLE = PrettyTable()\n",
    "  for c in obj.keys():\n",
    "    TABLE.add_column(c, [  ])\n",
    "  TABLE.add_row( [ obj[ ky ] for ky in obj.keys() ] )\n",
    "\n",
    "  print( f'{message}\\n' , TABLE )\n",
    "\n",
    "# Function for Histogram Equilisation\n",
    "def Image_equilise( X:dict , image:list )->( dict , list ):\n",
    "  \"\"\"_summary_\n",
    "    This returns the equalised image along with the frequency list of intersities for the calculation in the histogram. The procedure used here is the transformation of the discrete variable with the help of its cumulative distributive function.\n",
    "  Args:\n",
    "      X (dict): This is a dictionary which has contains the intensity value as the keys and their value are their frequency value.\n",
    "      image (list): this contains the vector of the original grayscale image file.\n",
    "\n",
    "  Returns:\n",
    "      ( list , list ): It returns a tuple which contains the frequency of the equalised image and the equalised image as a vector.\n",
    "  \"\"\"\n",
    "  data = {  'key':[], 'value':[]  }\n",
    "\n",
    "  for key in X:\n",
    "    data['key'].append(key)\n",
    "    data['value'].append(X[key])\n",
    "  summ = sum(data['value']);   ln = 256\n",
    "\n",
    "\n",
    "  data['pdf'] = [ value/summ for value in data['value']  ]\n",
    "  data['cdf'] = []; temp = 0\n",
    "  for i in range( ln ):\n",
    "    temp += data['pdf'][i]\n",
    "    data['cdf'].append(temp)\n",
    "\n",
    "  transform = [ round(255*data['cdf'][i]) for i in range( ln ) ]\n",
    "\n",
    "  EQ_pix_data = [ ]\n",
    "\n",
    "  XY_mapping = { i:transform[i] for i in range( ln ) }\n",
    "\n",
    "  equalized_image = []\n",
    "  for i in range( ln ):\n",
    "\n",
    "    temp = []\n",
    "    for j in range( ln ):\n",
    "\n",
    "      var = XY_mapping.get(image[i][j])\n",
    "      if var == None:\n",
    "        var = 0\n",
    "\n",
    "      temp.append( var )\n",
    "      EQ_pix_data.append(var)\n",
    "\n",
    "    equalized_image.append(temp)\n",
    "\n",
    "  return ( EQ_pix_data , equalized_image  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toNUPtfMPJmd"
   },
   "source": [
    "### __Code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "m_KDJDdUPJmd",
    "outputId": "990cd46a-a44c-493b-ae74-c7085e81db91"
   },
   "outputs": [],
   "source": [
    "# Reading the image\n",
    "org_image = cv2.imread('./test2.png' )\n",
    "plt.imshow(org_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "UeT6lYZzPJmd",
    "outputId": "18645d46-e7c4-42e8-da17-fcc8ba881400"
   },
   "outputs": [],
   "source": [
    "# Converting the image to gray-scale\n",
    "image = cv2.cvtColor(org_image , cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow( image )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXFFl5RaPJmd",
    "outputId": "e648c436-e3d2-462c-f749-f0b6dbc0c862"
   },
   "outputs": [],
   "source": [
    "pix = { i:0 for i in range(256)}\n",
    "\n",
    "pix_data = []\n",
    "for i in range(256):\n",
    "  for j in range(256):\n",
    "\n",
    "    pix[ image[i][j] ] += 1\n",
    "    pix_data.append(image[i][j])\n",
    "\n",
    "print(\"\" , pix )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARfkfg2fPJme"
   },
   "source": [
    "### __Plots__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GnKABz_yPJme",
    "outputId": "05042acc-43a4-48f8-85f9-768e3e8da925"
   },
   "outputs": [],
   "source": [
    "# Different bin sizes used to show impact on the visualization\n",
    "bin_size = [32 ,64 ,96 ,128]\n",
    "\n",
    "plt.figure( figsize=(14,14)  ); #plt.rcParams['figure.autolayout']=True\n",
    "for i in range( len( bin_size ) ):\n",
    "\n",
    "  # Making the condition for the plots\n",
    "  plt.style.use( 'default' )\n",
    "  plt.subplot(2,2,i+1)\n",
    "  plt.title(f\"Bin size={bin_size[i]}\")\n",
    "\n",
    "  # Showing the bar plot\n",
    "  plt.hist(pix_data, bins=bin_size[i], linewidth=1, edgecolor=\"white\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vRo7R6OGPJme",
    "outputId": "fd8fdbc2-5414-4ab1-9731-ef0be359fc0c"
   },
   "outputs": [],
   "source": [
    "# Applying histogram equalization using the equalization function made by self\n",
    "eq_data, equalized_image = Image_equilise( pix , image )\n",
    "\n",
    "# Plot the images and the original histograms\n",
    "plt.figure(figsize=(14, 14))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(pix_data, bins=64, range=[0, 256], color='gray')\n",
    "plt.title('Original Histogram')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(equalized_image, cmap='gray')\n",
    "plt.title('Equalized Image')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(eq_data, bins=64, range=[0, 256], color='gray')\n",
    "plt.title('Equalized Histogram')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-IzWFoUPJmf"
   },
   "source": [
    "### __Observations__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBaM5GvZPJmf"
   },
   "source": [
    "- #### The original intensity histogram of the image had a very high peak for a specific intensity value. This can be visualised in the image itself as the area leaving the girl and the dog is bright and has a similar value of intesity.\n",
    "- #### It is observed that the intensity histogram of the image after equalisation follows a near Uniform distribution.\n",
    "- #### But the image has an increase level of noise and it disrupts the information shown in the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIuB_U59PJmf"
   },
   "source": [
    "### __Results__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G98ggk4vPJmf"
   },
   "source": [
    "- #### In the case above, the equalisation leads to the creation of noise in the image which can lead us to the point that such type of images where a specific range of intensity value occur in the image' pixels should not be equalised. This could lead to the loss of information which is being shown in the image.\n",
    "- #### This also leads us to the fact that the equalisation of a bright image will lead to its darkening or many of its feature might become less observable.\n",
    "- #### Hence, it becomes important for the person processing the image to observe the intensity histogram and apply operations on the intensity depending on the need of the person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2t2s4BJIPJmf"
   },
   "source": [
    "---\n",
    "## __References__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGRTccYlPJmf"
   },
   "source": [
    "- #### [Help in Markdown](https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index)\n",
    "- #### [Matplotlib : Documentation](https://matplotlib.org/stable/index.html)\n",
    "- #### [OpenCV Tutorials and Documentation](https://docs.opencv.org/3.4/d6/d00/tutorial_py_root.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
