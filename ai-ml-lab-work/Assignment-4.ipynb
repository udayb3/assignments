{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yR4I8NY5e2_"
   },
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PTReiOw-RAN"
   },
   "source": [
    "# Advantage Actor Critic (A2C)\n",
    "\n",
    "\n",
    "- Here, We're going **to train a robotic arm** (Franka Emika Panda robot) to perform a task:\n",
    "- `Reach`: the robot must place its end-effector at a target position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QInFitfWno1Q"
   },
   "source": [
    "### Environments:\n",
    "\n",
    "- [Panda-Gym](https://github.com/qgallouedec/panda-gym)\n",
    "\n",
    "### RL-Library:\n",
    "\n",
    "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTpYcVZVMzUI"
   },
   "source": [
    "## Create a virtual display üîΩ\n",
    "\n",
    "- During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install gymnasium\n",
    "!pip install huggingface_sb3\n",
    "!pip install huggingface_hub\n",
    "!pip install panda_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tQ6QQmNqzdB"
   },
   "source": [
    "## Initialising the Display class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ww5PQH1gNLI4"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display( visible=0, size = (1400,900) )\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1obkbdJ_KnG"
   },
   "source": [
    "### Install dependencies üîΩ\n",
    "\n",
    "The Following dependencies were previously installed.\n",
    "- `gymnasium`: Maintained fork of openAI's gym, providing the configurations for different environments.\n",
    "- `panda-gym`: Contains the robotics arm environments.\n",
    "- `stable-baselines3`: The SB3 deep reinforcement learning library.\n",
    "- `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face ü§ó Hub.\n",
    "- `huggingface_hub`: Library allowing anyone to work with the Hub repositories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTep3PQQABLr"
   },
   "source": [
    "## Import the packages üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpiB8VdnQ7Bk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfBwIS_oAVXI"
   },
   "source": [
    "## PandaReachDense-v3 ü¶æ\n",
    "\n",
    "The agent we're going to train is a robotic arm that needs to do controls (moving the arm and using the end-effector).\n",
    "\n",
    "In robotics, the *end-effector* is the device at the end of a robotic arm designed to interact with the environment.\n",
    "\n",
    "In `PandaReach`, the robot must place its end-effector at a target position (green ball).\n",
    "\n",
    "We're going to use the dense version of this environment. It means we'll get a *dense reward function* that **will provide a reward at each timestep** (the closer the agent is to completing the task, the higher the reward). Contrary to a *sparse reward function* where the environment **return a reward if and only if the task is completed**.\n",
    "\n",
    "Also, we're going to use the *End-effector displacement control*, it means the **action corresponds to the displacement of the end-effector**. We don't control the individual motion of each joint (joint control).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/robotics.jpg\"  alt=\"Robotics\"/>\n",
    "\n",
    "\n",
    "This way **the training will be easier**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frVXOrnlBerQ"
   },
   "source": [
    "### Create the environment\n",
    "\n",
    "#### The environment üéÆ\n",
    "\n",
    "In `PandaReachDense-v3` the robotic arm must place its end-effector at a target position (green ball)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXzAu3HYF1WD"
   },
   "outputs": [],
   "source": [
    "env_id = \"PandaReachDense-v3\"\n",
    "\n",
    "# Create the env\n",
    "env = gym.make(env_id)\n",
    "s_size = env.observation_space.shape\n",
    "a_size = env.action_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mUKifa1qzdH"
   },
   "source": [
    "## Showing a random sample for the state and action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-U9dexcF-FB"
   },
   "outputs": [],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ib1Kxy4AF-FC"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_JClfElGFnF"
   },
   "source": [
    "The observation space **is a dictionary with 3 different elements**:\n",
    "- `achieved_goal`: (x,y,z) the current position of the end-effector.\n",
    "- `desired_goal`: (x,y,z) the target position for the end-effector.\n",
    "- `observation`: position (x,y,z) and velocity of the end-effector (vx, vy, vz).\n",
    "---\n",
    "- Given it's a dictionary as observation, We will use a MultiInputPolicy policy instead of MlpPolicy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5sXcg469ysB"
   },
   "source": [
    "### Normalize observation and rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZyX6qf3Zva9"
   },
   "source": [
    "A good practice in reinforcement learning is to [normalize input features](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).\n",
    "\n",
    "For that purpose, there is a wrapper that will compute a running average and standard deviation of input features.\n",
    "\n",
    "We also normalize rewards with this same wrapper by adding `norm_reward = True`\n",
    "\n",
    "[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RsDtHHAQ9Ie"
   },
   "outputs": [],
   "source": [
    "env = make_vec_env( env_id, n_envs=4)\n",
    "\n",
    "# Adding this wrapper to normalize the observation and the reward\n",
    "new_env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JmEVU6z1ZA-"
   },
   "source": [
    "### Creating the A2C Model\n",
    "\n",
    "For more information about A2C implementation with StableBaselines3 check: https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vR3T4qFt164I"
   },
   "outputs": [],
   "source": [
    "model = A2C(\"MultiInputPolicy\",env,  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opyK3mpJ1-m9"
   },
   "source": [
    "### Train the A2C agent üèÉ\n",
    "- Training our agent for 1,000,000 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TuGHZD7RF1G"
   },
   "outputs": [],
   "source": [
    "model.learn(1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfYtjj19cKFr"
   },
   "outputs": [],
   "source": [
    "# Save the model and  VecNormalize statistics when saving the agent\n",
    "model.save(\"a2c-PandaReachDense-v3\")\n",
    "new_env.save(\"vec_normalize.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01M9GCd32Ig-"
   },
   "source": [
    "### Evaluate the agent\n",
    "- Now that's our  agent is trained, we need to **check its performance**.\n",
    "- Stable-Baselines3 provides a method to do that: `evaluate_policy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liirTVoDkHq3"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Load the saved statistics\n",
    "eval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\n",
    "eval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n",
    "\n",
    "# We need to override the render_mode\n",
    "eval_env.render_mode = \"rgb_array\"\n",
    "\n",
    "eval_env.training = False\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "# Load and Evaluating the agent\n",
    "model = A2C.load(\"a2c-PandaReachDense-v3\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lgOp2cE4G5V"
   },
   "source": [
    "Actor-Critic (AC) algorithms give advantage over DQN, generally in complex environments with continuous action spaces and where the variance in learning needs to be minimized.\n",
    "1. Continuous Space limitation\n",
    "  - DQNs are typically limited to discrete action spaces because they rely on choosing actions by maximizing Q-values for each possible action in a finite set. Custom modifications are to be done to the environment space to use them in continuous space.\n",
    "  - Actor-Critic algo. handle continuous action spaces because they use a separate actor network to directly output actions instead of evaluating them individually.\n",
    "\n",
    "2. Reduced Overestimation Bias in Q-Value Estimation\n",
    "  - DQNs can suffer from overestimation bias, as the Q-learning target maximizes over Q-values, which can increase the estimation errors.\n",
    "  - Actor-Critic methods usually combine value-based and policy-based methods, helping in reducing bias and leading to more stable learning.\n",
    "\n",
    "- However, DQN is generally more data efficient than ActorCritic. When operating in action spaces that are small, the primitive exploration mechanism of DQN is often good enough to get results but they are very vulnerable to the continuous spaces.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
