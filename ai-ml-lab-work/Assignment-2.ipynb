{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELSclFBlADr2"
   },
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr9JNwm5L3Zs"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mAdCtrVL6XE"
   },
   "source": [
    "- In The Assignment, I have used the Indic2Transformer model and applied evaluation metrics BLEU and rogue-score on the inference from the model.\n",
    "\n",
    "- For this task, I used the test dataset IN22-GEN and choosing source language Assamese(asm_Beng) to target language Bengali(ben_BENG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKGyBzkKOb6P"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LFdniPVOerP"
   },
   "source": [
    "- The Dataset used is the test dataset __IN22-GEN__ and the files __test.asm_Beng__ and __test.ben_Beng__.\n",
    "- This dataset consists of 1024 sentences in all the 22 languages, for whom batch size was taken to be 4 and 16 during 2 such instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhjRbhtzPj9F"
   },
   "source": [
    "# Evaluation Metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp_uwOPMPmC0"
   },
   "source": [
    "## __BLEU metric__\n",
    "It is a evaluation metric used for tasks related to translations done by machine, when translating 1 language to other.\n",
    "\n",
    "__Workings__: The score find similarity between predicted and target text using n-grams, which are continuous sequences of n words. The precision is then modified by a brevity penalty to account for translations that are shorter than the targeted translations.\n",
    "\n",
    "__formula__:\n",
    "\n",
    "$$ BLEU = Brevity \\;\\; Penalty * e^{âˆ‘ pn} $$\n",
    "\n",
    "Where:\n",
    "- Brevity Penalty adjusts the score for translations which are shorter than the reference translations.\n",
    "\n",
    "- The range of BLEU score is from 0 to 1, with higher values indicating better translation.\n",
    "\n",
    "## __ROUGE scores__\n",
    "It is a set of metrices used for text summarization tasks, where we want to generate a summary of a longer text. It was designed to evaluate the quality of predicted summaries by comparing them to target summaries.\n",
    "\n",
    "__Working__\n",
    "\n",
    "It measures the similarity between the predicted summary and target summaries using _overlapping n-grams, in predicted summary and the target summaries. Most common n-grams used are unigrams, bigrams, and trigrams. It calculates the recall of n-grams in the predicted summary by comparing them to the target summaries.\n",
    "\n",
    "__Formula__:\n",
    "\n",
    "$$ ROUGE = \\Sigma{ (Recall \\; of \\; n\\;grams) } $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- Recall of n-grams: no. of n-grams which appear in both the predicted summary and the target summary divided by the total number of n-grams in the target summary.\n",
    "\n",
    "ROUGE score ranges from 0 to 1, with larger values showing better summary quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me8hQNQIMt4b"
   },
   "source": [
    "# Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR7Gy9xKMweC"
   },
   "source": [
    "I performed the generation and evaluation using the following steps:\n",
    "1. Following all the important steps for using the IndicTrans2 model used in the previous lab session, I used the pre-built method with the change in batch size from 4 to 16.\n",
    "2. We select T4 GPU as the run-time environment.\n",
    "3. During this procedure, I have used assamese and bengali whose test files, I imported in google Collab.\n",
    "4. Next, We obtain the tokenizer, model and find the predicted translation by the model.\n",
    "5. At last, I have used _BLEU_ and _rouge-scorer_ metrices for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8F5DlmwPYLS"
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cfsv02IeP2It"
   },
   "source": [
    "## Necessary Step\n",
    "\n",
    "Please run the cells below to install the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKcYlUZYGLrt"
   },
   "outputs": [],
   "source": [
    "# Clone the required Git repository for IndicTrans2\n",
    "%%capture\n",
    "!git clone https://github.com/AI4Bharat/IndicTrans2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3vs7FkIGSxK"
   },
   "outputs": [],
   "source": [
    "# Clone the Hugging face interface from github\n",
    "%%capture\n",
    "%cd /content/IndicTrans2/huggingface_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddkRAXQ2Git0"
   },
   "outputs": [],
   "source": [
    "# Install other essential dependecies for working of the transformer\n",
    "%%capture\n",
    "!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n",
    "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
    "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
    "!python3 -m pip install sentencepiece\n",
    "!python3 -m pip install torchmetrics datasets\n",
    "\n",
    "!git clone https://github.com/VarunGumma/IndicTransTokenizer\n",
    "%cd IndicTransTokenizer\n",
    "!python3 -m pip install --editable ./\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjN7ub1tO33H"
   },
   "source": [
    "Restart your run-time first and then run the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SLBIw6rQB-0"
   },
   "source": [
    "## Working for Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNM92xDow72u"
   },
   "source": [
    "1. Importing the important modules:\n",
    "  * transformer\n",
    "  * torch\n",
    "  * AutoModelForSeq2SeqLM from transformer\n",
    "  * BitsAndBytesConfig from transformer\n",
    "  * IndicProcessor from from IndicTransTokenizer\n",
    "  * IndicTransTokenizer from IndicTransTokenizer\n",
    "\n",
    "2. Set the Batch size equal to 8. Next, create a variable DEVICE and set it to \"cuda\" if torch.cuda.is_available() or else set it as \"cpu\". Finally set Quantization as \"None\"\n",
    "\n",
    "3. Two functions are there.\n",
    "    * First function to intialise the model and the tokenizer and returns both\n",
    "    * Another function which helps in the translation of a whole batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRfv0xszaQfx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForSeq2SeqLM\n",
    "from IndicTransTokenizer import IndicProcessor, IndicTransTokenizer\n",
    "import transformers as trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYczM2U6G1Zv"
   },
   "outputs": [],
   "source": [
    "# Set the variables\n",
    "BATCH_SIZE= 8;\n",
    "DEVICE= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "Quantization= None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLnr2RfM0QTt"
   },
   "source": [
    "### Model initializer and tokenizer function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIC98HYa0XRN"
   },
   "source": [
    "We used the function initialize_model_and_tokenizer which takes in 3 arguments: ckpt_dir, direction, quantization.\n",
    "\n",
    "Create a variable tokenizer.\n",
    "\n",
    "Next, We create a model variable set to AutoModelForSeq2SeqLM where we have to load the pretrained model from checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xj1WCNjuHG-d"
   },
   "outputs": [],
   "source": [
    "# Create a function initialize_model_and_tokenizer which takes in 4 arguments: ckpt_dir, direction, quantization.\n",
    "def initialize_model_and_tokenizer(ckpt_dir, direction, quantization):\n",
    "    if (quantization  == '4-bit'):\n",
    "      qconfig= BitsAndBytesConfig(\n",
    "          load_in_4bit=True,\n",
    "          bnb_4bit_use_double_quant= True,\n",
    "          bnb_4bit_compute_dtype= torch.bfloat16\n",
    "    )\n",
    "    elif (quantization  == '8-bit'):\n",
    "      qconfig= BitsAndBytesConfig(\n",
    "          load_in_8bit=True,\n",
    "          bnb_8bit_use_double_quant= True,\n",
    "          bnb_8bit_compute_dtype= torch.bfloat32\n",
    "    )\n",
    "    else:\n",
    "      qconfig=None\n",
    "\n",
    "    # Create a variable tokenizer and set it as IndicTransTokenizer with direction set as direction.\n",
    "    tokenizer= IndicTransTokenizer( direction=direction )\n",
    "\n",
    "    # Create a model variable set to AutoModelForSeq2SeqLM, Keep trust_remote_code=True, low_cpu_mem_usage=True and quantization_config=qconfig.\n",
    "    model= AutoModelForSeq2SeqLM.from_pretrained(ckpt_dir, trust_remote_code=True, low_cpu_mem_usage=True, quantization_config=qconfig)\n",
    "\n",
    "\n",
    "    # if qconfig is none, save the model in device.\n",
    "    if qconfig == None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    model.eval();\n",
    "    # return both tokenizer and model\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCWrirPAF3aM"
   },
   "source": [
    "## Helper Function to get translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPlUvJN1CX0K"
   },
   "outputs": [],
   "source": [
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    translations = []\n",
    "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
    "        batch = input_sentences[i : i + BATCH_SIZE]\n",
    "\n",
    "        # Preprocess the batch and extract entity mappings\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "        # Tokenize the batch and generate input encodings\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            src=True,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Generate translations using the model\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=0,\n",
    "                max_length=256,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens into text\n",
    "        generated_tokens = tokenizer.batch_decode(generated_tokens.detach().cpu().tolist(), src=False)\n",
    "\n",
    "        # Postprocess the translations, including entity replacement\n",
    "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
    "\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsosjO0PJJsU"
   },
   "source": [
    "## Languages and their codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X644TodGGkBE"
   },
   "source": [
    "Now we have to Finally join all the functions and datasets together to create our own predictions.\n",
    "\n",
    "Here is the list of languages supported by the IndicTrans2 models:\n",
    "\n",
    "| Language                       | Code      |\n",
    "|--------------------------------|-----------|\n",
    "| Assamese                       | asm_Beng  |\n",
    "| Bengali                        | ben_Beng  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rH-mfoBa_WPM"
   },
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1724235318620,
     "user": {
      "displayName": "Uday Bhardwaj",
      "userId": "11836933945904674808"
     },
     "user_tz": -330
    },
    "id": "HtGev7pW8T06",
    "outputId": "edef685b-36e5-4a2b-865b-a200b0b302b8"
   },
   "outputs": [],
   "source": [
    "with open('/content/test.asm_Beng', 'r') as f:\n",
    "    input_assamese = f.readlines()\n",
    "    input_assamese= [line.strip() for line in input_assamese]\n",
    "\n",
    "with open('/content/test.ben_Beng', 'r') as f:\n",
    "    target_bengali = f.readlines()\n",
    "    target_bengali= [line.strip() for line in target_bengali]\n",
    "\n",
    "input_assamese[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1724235324462,
     "user": {
      "displayName": "Uday Bhardwaj",
      "userId": "11836933945904674808"
     },
     "user_tz": -330
    },
    "id": "13vjIQtrR7CH",
    "outputId": "53acaf5a-3b0b-4bce-9db4-d3ee274b166e"
   },
   "outputs": [],
   "source": [
    "target_bengali[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiMcGBvPD5qN"
   },
   "source": [
    "## Assamese to Bengali conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 680420,
     "status": "ok",
     "timestamp": 1724236085903,
     "user": {
      "displayName": "Uday Bhardwaj",
      "userId": "11836933945904674808"
     },
     "user_tz": -330
    },
    "id": "x86sXfN6EEx_",
    "outputId": "26fd56bf-d1d7-4f4b-dc6e-b65fc2119e3c"
   },
   "outputs": [],
   "source": [
    "AI4_BHARAT= \"ai4bharat/indictrans2-indic-indic-1B\"; my_direction=\"/content/IndicTrans2/huggingface_interface/IndicTransTokenizer/IndicTransTokenizer/indic-indic/\";\n",
    "\n",
    "# getting tokenizer and model by passing arg. to initialize_model_and_tokenizer function\n",
    "Tokenizer, model= initialize_model_and_tokenizer( ckpt_dir=AI4_BHARAT, direction= my_direction, quantization=\"4-bit\"  )\n",
    "\n",
    "indic= IndicProcessor(inference=True)\n",
    "\n",
    "# Choose the source langauge as English and target language as Hindi.\n",
    "lan_src= \"asm_Beng\"; lan_tar=\"ben_Beng\"\n",
    "\n",
    "# Find target translation using the batch_translate function with arguments: input_sentences, src_lang, tgt_lang, model, tokenizer, ip\n",
    "pred_bengali= batch_translate( input_assamese , lan_src, lan_tar, model, Tokenizer, indic  )\n",
    "\n",
    "del Tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1069,
     "status": "ok",
     "timestamp": 1724236388716,
     "user": {
      "displayName": "Uday Bhardwaj",
      "userId": "11836933945904674808"
     },
     "user_tz": -330
    },
    "id": "KhD2Va8_EDsf",
    "outputId": "52403c04-a464-4986-b0eb-c65fcf2622ae"
   },
   "outputs": [],
   "source": [
    "pred_bengali[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJYpS_RyQbrI"
   },
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1349,
     "status": "ok",
     "timestamp": 1724240455294,
     "user": {
      "displayName": "Uday Bhardwaj",
      "userId": "11836933945904674808"
     },
     "user_tz": -330
    },
    "id": "oSFWlT7hj-xF",
    "outputId": "6c477bb6-92d4-431d-854c-676107bd0376"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "pred_new= [ sen.split() for sen in pred_bengali ]\n",
    "tar_new= [[sen.split(' ')] for sen in target_bengali ]\n",
    "\n",
    "metric= load_metric(\"bleu\")\n",
    "metric.add_batch(predictions= pred_new, references=tar_new)\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1222,
     "status": "ok",
     "timestamp": 1724240465301,
     "user": {
      "displayName": "Uday Bhardwaj",
      "userId": "11836933945904674808"
     },
     "user_tz": -330
    },
    "id": "w11dNMrufwO9",
    "outputId": "cf35be80-e905-4300-c388-2a1487d33bb2"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric= load_metric(\"rouge\")\n",
    "metric.add_batch(predictions= pred_bengali, references= target_bengali)\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpu3v8yPQwg7"
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6dz7lxcQz5x"
   },
   "source": [
    "These are some of the observations and inferences which I gained while working on the problem.\n",
    "  - There is a slight difference in the structure of the predicted translations and the target translations.\n",
    "  - The Evaluation metrics used showed very poor performance of the model when used for evaluating translation from Assamese to Bengali.\n",
    "  - The above result can happen due to different reasons such as the differnce in structuring of the words.\n",
    "  - A Better approach could use some different kind of preprocesssing which can improve the performance of the model.\n",
    "  - The accuracy can see a increase after fine-tuning the model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "vr9JNwm5L3Zs",
    "FKGyBzkKOb6P",
    "VhjRbhtzPj9F",
    "Me8hQNQIMt4b",
    "q8F5DlmwPYLS",
    "Cfsv02IeP2It",
    "_SLBIw6rQB-0",
    "qLnr2RfM0QTt",
    "kCWrirPAF3aM",
    "dsosjO0PJJsU",
    "rH-mfoBa_WPM",
    "eiMcGBvPD5qN",
    "lpu3v8yPQwg7"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
