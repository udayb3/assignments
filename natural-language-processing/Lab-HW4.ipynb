{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwmIdFWFKPYc"
   },
   "source": [
    "# Lab HW 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv7_eiM6Y_uB"
   },
   "source": [
    "# Question\n",
    "## Train an RNN and an LSTM model for two different tasks:\n",
    "  - ## Task 1: Language Modeling\n",
    "  - ## Task 2: Sentiment Analysis\n",
    "\n",
    "## Compare the performance of RNN and LSTM models for each task using suitable evaluation metrics.\n",
    "## For example, compare the perplexity values in case of language modeling; and accuracy, F1 score for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA5BC53d5jNv"
   },
   "source": [
    "# Language Modeling Task\n",
    "\n",
    "- Language modeling means using the various statistical/probabilistic techniques to determine the probability of a given sequence of words occurring in a sentence.\n",
    "\n",
    "- For this, I have used the [Movie Plots](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots) datasets taken directly from wikipedia. However, The Dataset was too large so i have only used 15 randomly selected movies for training and testing of the RNN and LSTM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz5ycdksFeFl"
   },
   "outputs": [],
   "source": [
    "# importing all the required modules and libraries\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7Ca7sz088aC",
    "outputId": "9d880b1b-4e9d-4a9c-c659-5901a83d9426"
   },
   "outputs": [],
   "source": [
    "# importing the movie plots dataset\n",
    "!kaggle datasets download -d jrobischon/wikipedia-movie-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KaqxgsZ79SwQ",
    "outputId": "4ef6ca78-7450-4b77-da53-6500407da1a4"
   },
   "outputs": [],
   "source": [
    "# unzupping the file and reading data from it\n",
    "path_to_zip_file = \"/content/wikipedia-movie-plots.zip\"\n",
    "directory_to_extract_to = \"/content/\"\n",
    "\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "  zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "df = pd.read_csv(\"/content/wiki_movie_plots_deduped.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVopMLgc-EaS"
   },
   "outputs": [],
   "source": [
    "# selecting 15 random movies to be used\n",
    "random_seq= df[\"Plot\"].sample(15)\n",
    "train_sen, test_sen= random_seq[:12], random_seq[12:]\n",
    "train_txt= \"\"\n",
    "for i in train_sen:\n",
    "  train_txt += i + \" \"\n",
    "\n",
    "test_txt= \"\"\n",
    "for i in test_sen:\n",
    "  test_txt += i + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mai5Ocml-q86",
    "outputId": "a578e94f-2b5f-477c-d232-4f7a57b9971b"
   },
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text\n",
    "train_tokens = tokenizer.tokenize(train_txt)\n",
    "test_tokens= tokenizer.tokenize(test_txt)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "train_ids = tokenizer.convert_tokens_to_ids(train_tokens)\n",
    "test_ids = tokenizer.convert_tokens_to_ids(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FEjfRYxWFEL",
    "outputId": "3754d28f-e7ae-4a6c-bf4a-ee050b10a233"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDswfOCWVrkM"
   },
   "outputs": [],
   "source": [
    "# here if i have created the usage class over the Dataset class to customize the __getitem__ and __len__ methods to fit my dataset\n",
    "class Usage( Dataset ):\n",
    "  def __init__(self, data, length):\n",
    "    self.data = torch.tensor(data, dtype=torch.long)\n",
    "    self.length = length\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return (self.data[idx:idx + self.length],self.data[idx + 1:idx + self.length + 1])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len( self.data ) - self.length\n",
    "\n",
    "# initilising the size of sequence and the number of sequences in a batch\n",
    "seq_len = 30; batch_size = 64\n",
    "\n",
    "train_dataset = Usage( train_ids, seq_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mka6gyvHWBYr"
   },
   "outputs": [],
   "source": [
    "# here i have defined classes for the language model\n",
    "class RNN_LM(nn.Module):\n",
    "\n",
    "    def __init__(self, vsize, embedding_size, hidden_size, layers):\n",
    "\n",
    "        super(RNN_LM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vsize, embedding_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vsize)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x)\n",
    "        out, h = self.rnn(x, h)\n",
    "        out = self.fc(out)\n",
    "        return out, h\n",
    "\n",
    "class LSTM_LM(nn.Module):\n",
    "\n",
    "    def __init__(self, vsize, embedding_size, hidden_size, layers):\n",
    "        super(LSTM_LM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vsize, embedding_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vsize)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x)\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        out = self.fc(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYdJ3-weWYEU"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, vsize, n_epochs=4, learning_rate= 0.0004):\n",
    "\n",
    "    loss_criterion = nn.CrossEntropyLoss();  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch, (input_data, target_data) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            h = None\n",
    "            output, h = model(input_data, h)\n",
    "\n",
    "            loss = loss_criterion(output.view(-1, vocab_size), target_data.view(-1))\n",
    "            loss.backward();  optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Step [{batch}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Average Loss till now: {total_loss / len(dataloader):.4f}')\n",
    "\n",
    "\n",
    "def find_perplexity(model, data_loader, vocab_size):\n",
    "    model.eval()\n",
    "    # setting and criterion as the cross entropy loss\n",
    "    total_loss = 0; criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_data, target_data in data_loader:\n",
    "            output, _ = model(input_data, None)\n",
    "            loss = criterion(output.view(-1, vocab_size), target_data.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return np.exp(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJWuwoGgIeIN"
   },
   "outputs": [],
   "source": [
    "test_dataset = Usage(test_ids, seq_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "embed_size = 128; hidden_size = 256\n",
    "layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqkTLMetJFnT"
   },
   "source": [
    "# Finding Perplexity for RNN Model: This will take around minutes for running as the plot for particular movie is generally very large in wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hy2zWmrOIZqB",
    "outputId": "36ad4476-7576-4241-f1a1-9277a0557a7d"
   },
   "outputs": [],
   "source": [
    "# Train RNN Model\n",
    "rnn_model = RNN_LM(vocab_size, embed_size, hidden_size, layers)\n",
    "train(rnn_model, train_loader, vocab_size)\n",
    "\n",
    "# Perplexity for RNN\n",
    "rnn_perplexity = find_perplexity(rnn_model, test_loader, vocab_size)\n",
    "print(f'\\nRNN Perplexity:\\n {rnn_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rouRh3POI2cO"
   },
   "source": [
    "# Finding Perplexity for the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7aB894bBYcQh",
    "outputId": "7bd165f3-e0a2-4d55-e18a-583a17b355b1"
   },
   "outputs": [],
   "source": [
    "# Train LSTM Model\n",
    "lstm_model = LSTM_LM(vocab_size, embed_size, hidden_size, layers)\n",
    "train(lstm_model, train_loader, vocab_size)\n",
    "\n",
    "# Perplexity for LSTM\n",
    "lstm_perplexity = find_perplexity(lstm_model, test_loader, vocab_size)\n",
    "print(f'LSTM Perplexity: {lstm_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fIAzrQpuLEF"
   },
   "source": [
    "# Results\n",
    "- RNN gave lesser value of Perplexity for the Language Modeling task than LSTM.\n",
    "- The Perplexity can be further improved if we use the complete dataset instead of using merely 30 sentences but that would have taken a lot of amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG50I8OIYfwe"
   },
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "- Here, We try to obtain the different sentiments related to different texts.\n",
    "- For this, I have used the [sentiment analysis](https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset) dataset from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpsQlD6tj9A6",
    "outputId": "d2a6ae1c-941f-488d-d4f3-c2f44778a803"
   },
   "outputs": [],
   "source": [
    "# downloading the dataset from kaggle\n",
    "!kaggle datasets download -d abhi8923shriv/sentiment-analysis-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfkuVMJtkIW4",
    "outputId": "95594424-5818-41a3-bee5-681a51d7bd3a"
   },
   "outputs": [],
   "source": [
    "# unzipping the file and separating the train and test data\n",
    "path_to_zip_file = \"/content/sentiment-analysis-dataset.zip\"\n",
    "directory_to_extract_to = \"/content/\"\n",
    "\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "  zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "tottrain_dt= pd.read_csv('train.csv', encoding= 'latin1');\n",
    "tottest_dt= pd.read_csv('test.csv', encoding='latin1', skip_blank_lines= True)\n",
    "\n",
    "print(f\"The lenth of the train set and test set are {len(tottrain_dt)} and {len(tottest_dt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-6w0jw0mNGm"
   },
   "outputs": [],
   "source": [
    "# converting the data to a proper format\n",
    "train_input= tottrain_dt['text'].to_numpy()\n",
    "train_target_ini= tottrain_dt['sentiment'].to_numpy()\n",
    "test_input= tottest_dt['text'].to_numpy()\n",
    "test_target_ini= tottest_dt['sentiment'].to_numpy()\n",
    "\n",
    "map_label= {'positive': 2, 'negative': 0, 'neutral': 1, np.nan: 1}\n",
    "map_fun= np.vectorize( lambda x: map_label[x] )\n",
    "train_target= map_fun(train_target_ini)\n",
    "test_target= map_fun(test_target_ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKskedDGnKoi",
    "outputId": "3c8342fa-4316-43b4-fc19-99d4d4c181de"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# making the class Usage_SA to convert the initial data to proper formats\n",
    "class Usage_SA(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        # Ensure text is a string\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize the pre-trained Bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 50;  batch_size = 32\n",
    "\n",
    "train_data = Usage_SA(train_input, train_target, tokenizer, max_length)\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Usage_SA(test_input, test_target, tokenizer, max_length)\n",
    "testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HehQ7nxznTlx"
   },
   "outputs": [],
   "source": [
    "# Creating the RNN and LSTM classes for using neural network module of pytorch\n",
    "class RNN_SA(nn.Module):\n",
    "\n",
    "    def __init__(self, vsize, embedding_size, hsize, osize):\n",
    "\n",
    "        super(RNN_SA, self).__init__()\n",
    "        self.embedding = nn.Embedding(vsize, embedding_size)\n",
    "        self.rnn = nn.RNN(embedding_size, hsize, batch_first=True)\n",
    "        self.fc = nn.Linear(hsize, osize)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        final_hidden_state = rnn_out[:, -1, :]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LSTM_SA(nn.Module):\n",
    "    def __init__(self, vsize, embedding_size, hsize, osize):\n",
    "\n",
    "        super( LSTM_SA, self).__init__()\n",
    "        self.embedding = nn.Embedding(vsize, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hsize, osize)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        final_hidden_state = lstm_out[:, -1, :]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKwdpBK4oiLI"
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "def train(model, dataloader, num_epochs=4, lr=0.0001):\n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for num, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['label']\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step: [{num}/{len(dataloader)}], Loss: {total_loss / len(dataloader):.4f}')\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}')\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['label']\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return accuracy, f1\n",
    "\n",
    "# Define model parameters\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "output_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1S6qUJQMhy1"
   },
   "source": [
    "# Training and Evaluating the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46WK--ElupES",
    "outputId": "9d7630e4-f78e-4830-85c2-bc9857b978ef"
   },
   "outputs": [],
   "source": [
    "# Train RNN Model\n",
    "rnn_model = RNN_SA( vocab_size, embed_size, hidden_size, output_size)\n",
    "train(rnn_model, trainloader)\n",
    "\n",
    "# Evaluate RNN Model\n",
    "rnn_accuracy, rnn_f1 = evaluate_model(rnn_model, testloader)\n",
    "print(f'RNN Model Accuracy: {rnn_accuracy:.4f}, F1 Score: {rnn_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lt_uNiYxMlSD"
   },
   "source": [
    "# Training and Evaluating the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IUTdeeBMgMw",
    "outputId": "2c83c5de-91d8-4635-8109-130677fb49e4"
   },
   "outputs": [],
   "source": [
    "# Train LSTM Model\n",
    "lstm_model = LSTM_SA(vocab_size, embed_size, hidden_size, output_size)\n",
    "train(lstm_model, trainloader)\n",
    "\n",
    "# Evaluate LSTM Model\n",
    "lstm_accuracy, lstm_f1 = evaluate_model(lstm_model, testloader)\n",
    "print(f'LSTM Model Accuracy: {lstm_accuracy:.4f}, F1 Score: {lstm_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_mkUlDmMqDq"
   },
   "source": [
    "# Results\n",
    "\n",
    "- For the task of Sentiment Analysis, both of the model gave similar scores, however this score can be further increased by performing data precprocessing on the data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
