{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab HW 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSxiwjW37zCr"
   },
   "source": [
    "### **Question: In the context of neural machine translation (NMT), consider a sequence-to-sequence LSTM model translating from English to Spanish. You are asked to evaluate the model performance under different settings:**\n",
    "\n",
    "> **Train the model both with and without attention mechanisms.**\n",
    "\n",
    "> **During inference, experiment with greedy decoding vs. beam search decoding.**\n",
    "\n",
    "> **Compare the results with and without teacher forcing during training.**\n",
    "\n",
    "> **For each of the following combinations, describe how the model's performance might change and explain why:**\n",
    "\n",
    ">>> *No attention, greedy decoding, with teacher forcing*\n",
    "\n",
    ">>> *Attention, beam search decoding, with teacher forcing*\n",
    "\n",
    ">>> *Attention, greedy decoding, without teacher forcing*\n",
    "\n",
    ">>> *No attention, beam search decoding, without teacher forcing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlnR_g-Rqycm"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBM6x97spE1P",
    "outputId": "b2a87d41-ec00-40a3-c4e1-5089d572de4c"
   },
   "outputs": [],
   "source": [
    "Bisen = []\n",
    "\n",
    "# Download the dataset\n",
    "data_file = keras.utils.get_file( fname=\"spa-eng.zip\", origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\", extract=True )\n",
    "data_file = pathlib.Path( data_file ).parent / \"spa-eng\" / \"spa.txt\"\n",
    "# Prepare the dataset\n",
    "with open(data_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "for line in lines:\n",
    "    sen_en, sen_sp = line.split(\"\\t\");    sen_sp = \"[start] \" + sen_sp + \" [end]\"\n",
    "    Bisen.append( (sen_en, sen_sp) )\n",
    "\n",
    "random.shuffle(Bisen)\n",
    "num_val_samples = int(0.15 * len(Bisen))\n",
    "num_train_samples = len(Bisen) - 2 * num_val_samples\n",
    "train_pairs = Bisen[:num_train_samples]\n",
    "val_pairs = Bisen[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = Bisen[(num_train_samples + num_val_samples):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_AtvcwBpbz1"
   },
   "source": [
    "# Text Vectorization\n",
    "We'll vectorize the English and Spanish sentences, limiting the vocabulary size and sequence length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Msi_L00WpaoI"
   },
   "outputs": [],
   "source": [
    "# Vectorizing the text data\n",
    "strip_chars = string.punctuation + \"Â¿\"; strip_chars = strip_chars.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "VSIZE = 15000;  sequence_length = 20\n",
    "BSIZE = 64\n",
    "\n",
    "def standardization(input_string):\n",
    "    lowercase = tf_strings.lower(input_string)\n",
    "    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "# English and spanish vectorisation\n",
    "vect_en = TextVectorization(  max_tokens=VSIZE,  output_mode=\"int\",  output_sequence_length=sequence_length )\n",
    "vect_sp = TextVectorization(  max_tokens=VSIZE,  output_mode=\"int\",   output_sequence_length=sequence_length + 1, standardize=standardization )\n",
    "\n",
    "# Adapt vectorization to the training data\n",
    "train_eng_texts = [pair[0] for pair in train_pairs];  train_spa_texts = [pair[1] for pair in train_pairs]\n",
    "vect_en.adapt(train_eng_texts); vect_sp.adapt(train_spa_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH0-0E7-pfvN"
   },
   "source": [
    "# Formatting the dataset for the model\n",
    "We format the dataset into pairs of encoder inputs (English) and decoder inputs (Spanish), which will be used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84MDfkpypggl"
   },
   "outputs": [],
   "source": [
    "def format_dataset(sen_en, sen_sp):\n",
    "    vec_en = vect_en(sen_en); vec_sp = vect_sp(sen_sp)\n",
    "    return ({ \"encoder_inputs\": vec_en, \"decoder_inputs\": vec_sp[:, :-1], }, vec_sp[:, 1:] )\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    line_en, line_sp = zip(*pairs)\n",
    "    line_en = list(line_en); line_sp = list(line_sp)\n",
    "\n",
    "    # slicing the dataset\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((line_en, line_sp)); dataset = dataset.batch(BSIZE)\n",
    "\n",
    "    # applying the format function on each of the dataset's row\n",
    "    return dataset.map(format_dataset).cache().shuffle(2048).prefetch(16)\n",
    "\n",
    "ds_train = make_dataset(train_pairs); ds_vali = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzVJt-M2pjrN"
   },
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52zFMQHdpk5V"
   },
   "outputs": [],
   "source": [
    "class PEmbedding(layers.Layer):\n",
    "\n",
    "    def __init__(self, sequence_length, vsize, embed_dim, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(input_dim=vsize, output_dim=embed_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vsize\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        ln = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=ln, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs);  embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "      return None if mask is None else tf.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({ \"sequence_length\": self.sequence_length,  \"vocab_size\": self.vocab_size,  \"embed_dim\": self.embed_dim })\n",
    "        return config\n",
    "\n",
    "class TransEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        attention_output = self.attention(query=inputs, value=inputs, key=inputs)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "class TransDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, use_attention=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_attention = use_attention\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(latent_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        padding_mask = tf.minimum(tf.cast(mask[:, None, :], dtype=\"int32\"), causal_mask) if mask is not None else None\n",
    "\n",
    "        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        if self.use_attention:\n",
    "            attention_output_2 = self.attention_2(\n",
    "                query=out_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask\n",
    "            )\n",
    "            out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "        else:\n",
    "            out_2 = out_1\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, None]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        return tf.tile(tf.reshape(mask, (1, sequence_length, sequence_length)), [batch_size, 1, 1])\n",
    "\n",
    "def build_trans_model(use_attention=True):\n",
    "    embed_dim = 256\n",
    "    latent_dim = 2048\n",
    "    num_heads = 8\n",
    "\n",
    "    # Encoder\n",
    "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "    x = PEmbedding(sequence_length, VSIZE, embed_dim)(encoder_inputs)\n",
    "    encoder_outputs = TransEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "    x = PEmbedding(sequence_length, VSIZE, embed_dim)(decoder_inputs)\n",
    "\n",
    "    if use_attention:\n",
    "        x = TransDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "    else:\n",
    "        x = layers.Dense(latent_dim, activation=\"relu\")(x)\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense( VSIZE, activation=\"softmax\")(x)\n",
    "    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "    # Assembling full transformer model\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n",
    "    return transformer, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qC2HZJGAp2x-"
   },
   "source": [
    "# Evaluating the Model on 4 different settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXiCFIcZxnn2"
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "max_ln = 20\n",
    "spa_vocab = vect_sp.get_vocabulary(); spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37d7kTq9qP4L",
    "outputId": "5a15a397-3a31-469d-e3d0-f28abce6c007"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(transformer, input_sentence, use_greedy=True, beam_size=3):\n",
    "    tokenized_input_sentence = vect_en([input_sentence]);  decoded_sentence = \"[start]\"\n",
    "    for i in range(max_ln):\n",
    "        tokenized_target_sentence = vect_sp([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        if use_greedy:\n",
    "            sampled_token_index = tf.argmax(predictions[0, i, :]).numpy()\n",
    "        else:\n",
    "            top_k_indices = tf.argsort(predictions[0, i, :])[-beam_size:].numpy()\n",
    "            sampled_token_index = random.choice(top_k_indices)\n",
    "\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "\n",
    "def calculate_model(use_attention, use_greedy, beam_size=3, teacher_forcing=True):\n",
    "\n",
    "\n",
    "    transformer, encoder, decoder = build_trans_model(use_attention=use_attention)\n",
    "    transformer.compile( optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"] )\n",
    "    transformer.fit(ds_train, epochs=epochs, validation_data=ds_vali)\n",
    "\n",
    "    if teacher_forcing:\n",
    "        history = transformer.fit(ds_train, epochs=epochs, validation_data=ds_vali)\n",
    "    else:\n",
    "        history = transformer.fit(ds_train.map(lambda x, y: (x, x[\"decoder_inputs\"])), epochs=epochs, validation_data=ds_vali)\n",
    "    test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "    translations = []\n",
    "    for _ in range(6):\n",
    "        input_sentence = random.choice(test_eng_texts)\n",
    "        translated_sentence = decode_sequence(transformer, input_sentence, use_greedy=use_greedy, beam_size=beam_size)\n",
    "        translations.append((input_sentence, translated_sentence))\n",
    "\n",
    "    return history.history, translations\n",
    "\n",
    "print(\"No attention, greedy decoding with the use of teacher forcing\")\n",
    "history_1, translations_1 = calculate_model(use_attention=False, use_greedy=True, teacher_forcing=True)\n",
    "\n",
    "print(\"2. Attention, beam search decoding with the use of teacher forcing\")\n",
    "history_2, translations_2 = calculate_model(use_attention=True, use_greedy=False, beam_size=3, teacher_forcing=True)\n",
    "\n",
    "print(\"3. Attention, greedy decoding, without the use of teacher forcing\")\n",
    "history_3, translations_3 = calculate_model(use_attention=True, use_greedy=True, teacher_forcing=False)\n",
    "\n",
    "print(f\"{history_1}\\n{history_2}\\n{history_3}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
