{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8do1hUzdlPx"
   },
   "source": [
    "# Lab HW 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAHG_psOk2pb"
   },
   "source": [
    "# **Questions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GnnMJvygJ29",
    "outputId": "ef633adf-cb9e-4a7e-d4bb-e434e0d11b92"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgF-CUm1le5O"
   },
   "source": [
    "#**Q1) Write a program that parses a given word into its root form and affixes (prefixes and suffixes). The program should output the morphemes.**\n",
    "Example Input: \"unhappiness\"\n",
    "\n",
    "Example Output: [\"un-\", \"happy\", \"-ness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojjZ3XTApUQj"
   },
   "source": [
    "- Here, We have to handle words like happiness, craziness, competition whose root forms can not be obtained just by removing suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWPC2TczmrHw",
    "outputId": "12d139af-8cdf-4897-b2f8-e6039d847041"
   },
   "outputs": [],
   "source": [
    "def give_morphemes(word):\n",
    "    prefix = ['un', 're', 'in', 'dis', 'im', 'ir', 'il', ]\n",
    "    suffix = ['ed', 'ing', 'ly', 'ness', 's', 'er', 'able', 'ment', 'ition']\n",
    "\n",
    "    base_word = word; ans=[]; t1= None\n",
    "\n",
    "    # Checking for prefixes\n",
    "    for pref in prefix:\n",
    "        if word.startswith(pref):\n",
    "            base_word = word[len(pref):]\n",
    "            ans.append( pref + \"-\")\n",
    "            break\n",
    "\n",
    "    # Checking for suffixes\n",
    "    for suf in suffix:\n",
    "        if base_word.endswith(suf):\n",
    "            base_word = base_word[:-len(suf)]\n",
    "            t1= suf\n",
    "            break\n",
    "\n",
    "    # Handle special cases like 'happi' to 'happy'\n",
    "    if base_word.endswith('i') and word.endswith('ness'):\n",
    "        base_word = base_word[:-1] + 'y'\n",
    "\n",
    "    if word.endswith('ition'):\n",
    "      base_word = base_word + \"e\"\n",
    "\n",
    "    ans.append(base_word)\n",
    "    if t1 is not None:\n",
    "      ans.append(\"-\"+t1)\n",
    "    return ans\n",
    "\n",
    "# Example usage\n",
    "print( give_morphemes(\"unhappiness\") )\n",
    "print(give_morphemes(\"retry\") )\n",
    "print( give_morphemes(\"tryre\") )\n",
    "print(give_morphemes(\"craziness\"))\n",
    "print(give_morphemes('competition'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avb1UNkPme6M"
   },
   "source": [
    "#**Q2) Write a program that identifies the derivational morphemes in a given sentence and classifies each word into its base form and derivational affixes.**\n",
    "\n",
    "Example:\n",
    "Input Sentence: \"The artistically gifted boy performed exceptionally in the\n",
    "competition.\"\n",
    "\n",
    "Output:\n",
    "\n",
    "    \"word\": \"artistically\",\n",
    "    \"base_form\": \"art\",\n",
    "    \"derivational_affixes\": [\"-ist\", \"-ic\", \"-ally\"],\n",
    "  \n",
    "    \"word\": \"gifted\",\n",
    "    \"base_form\": \"gift\",\n",
    "    \"derivational_affixes\": [\"-ed\"],\n",
    "  \n",
    "    \"word\": \"performed\",\n",
    "    \"base_form\": \"perform\",\n",
    "    \"derivational_affixes\": [\"-ed\"],\n",
    "  \n",
    "    \"word\": \"exceptionally\",\n",
    "    \"base_form\": \"except\",\n",
    "    \"derivational_affixes\": [\"-ion\", \"-al\", \"-ly\"],\n",
    "  \n",
    "    \"word\": \"competition\",\n",
    "    \"base_form\": \"compete\",\n",
    "    \"derivational_affixes\": [\"-ition\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdyWB5A3ehW9"
   },
   "source": [
    "- This is a custom implemented method, here we first separate the sentence using \" \" and then apply the method `get_derivational_morphemes`.\n",
    "- For the method, we iterate from the back of the word and separate out the smallest affix which we can find in the word and start out again from new end of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8v-0p7_ohEJ",
    "outputId": "f7530370-1ab9-48bb-a9c5-d0e4be2458ae"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Derivational morphemes we are going to check\n",
    "derivational_affixes = ['ness','ally','ful','less','ment','er','est','ing','ed', 'ize','ation','al','ist','ic', 'ed', 'ion', 'ition']\n",
    "\n",
    "def get_derivational_morphemes(word):\n",
    "\n",
    "    affix = []\n",
    "    i= len(word)-1; j=len(word)\n",
    "    while(i>-1):\n",
    "      if word[i:j] in derivational_affixes:\n",
    "\n",
    "        if(word[i:j]=='ion' and word[i-2:i]=='it'): # handling the case for -ition\n",
    "          affix.append(\"-\"+word[i-2:j]);\n",
    "          i=i-2;\n",
    "        else:\n",
    "          affix.append(\"-\"+word[i:j]);\n",
    "\n",
    "        j=i\n",
    "\n",
    "      i-=1\n",
    "\n",
    "    return word[:j], affix\n",
    "\n",
    "def label_morphemes(sen):\n",
    "\n",
    "    results = []\n",
    "    doc= sen.split(); doc[-1].strip('.')\n",
    "\n",
    "    for word in doc:\n",
    "\n",
    "        base_form, affix = get_derivational_morphemes(word.lower())\n",
    "        if len(affix)>=1:\n",
    "            results.append({\n",
    "                'word': word,\n",
    "                'base_form': base_form,\n",
    "                'affix': affix[::-1]\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "sen = \"The artistically gifted boy performed exceptionally in the competition.\"\n",
    "results = label_morphemes(sen)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Word: {result['word']}, Base Form: {result['base_form']}, derivatonal_affixes: {result['affix']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dR657un_o4sD"
   },
   "source": [
    "#**Q3) Implement a word segmentation program that can handle ambiguous cases where multiple segmentations are possible. The program should use a probabilistic model to choose the most likely segmentation.**\n",
    "Example Input: \"therapistfinder\"\n",
    "\n",
    "Example Output: [\"therapist\", \"finder\"] or [\"the\", \"rapist\", \"finder\"] (depending on context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAgSBhUOfKLi"
   },
   "source": [
    "- A dynammic programming based approach is used to find the optimum word.\n",
    "- Optimum word implies here the the word which has the highest probability to come alone.\n",
    "- This is being done by giving the data for frequency of each word. I have taken frequencies to be random here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_beotLRHVV9C",
    "outputId": "3e240c1c-96aa-4ebe-b872-7de56a31be18"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class mySegmenter:\n",
    "\n",
    "  def __init__(self, sample_text):\n",
    "      self.probability = self._build_probab_table(sample_text)\n",
    "\n",
    "  def _build_probab_table(self, sample_text):\n",
    "      n = sum(sample_text.values() )\n",
    "      return { w: math.log(coun / n) for w, coun in sample_text.items()}\n",
    "\n",
    "  def segment_word(self, test_text):\n",
    "\n",
    "      sz:int = len(test_text)\n",
    "      dp_table = [None] * (sz + 1);  dp_table[0] = (0, [])\n",
    "\n",
    "      for i in range(1, sz + 1):\n",
    "        options = []\n",
    "        for j in range(i):\n",
    "          w = test_text[j:i]\n",
    "          if w in self.probability:\n",
    "            cur_prob = self.probability[w]\n",
    "            if dp_table[j]!=None:\n",
    "              options.append((dp_table[j][0] + cur_prob, dp_table[j][1] + [w]))\n",
    "\n",
    "        if(len(options)!=0):\n",
    "          dp_table[i] = max(options, key=lambda x: x[0]) # finding max values based on higher probability\n",
    "\n",
    "      return dp_table[sz][1] if dp_table[sz] else [test_text]\n",
    "\n",
    "# The given corpus is a dict with key as the word and its value as the number of occurences of it\n",
    "data = [{ \"the\": 1423,  \"therapist\": 23,  \"rapist\": 100,  \"finder\": 24, \"ist\": 201, \"istfinder\": 3, \"ther\":4, \"find\": 10  },\n",
    " { \"it\":  1524, \"grey\": 1000, \"isgrey\":  10, \"is\": 4000, \"ey\": 1000 , \"itis\":1 }\n",
    "]\n",
    "\n",
    "seg1 = mySegmenter(data[0])\n",
    "seg2 = mySegmenter(data[1])\n",
    "sample1 = \"therapistfinder\";  sample2= \"itisgrey\"\n",
    "\n",
    "output1 = seg1.segment_word(sample1)\n",
    "output2= seg2.segment_word(sample2)\n",
    "print(\"Segmented Words:\", output1)\n",
    "print(\"Segmented Words:\", output2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk92vz3bVWeh"
   },
   "source": [
    "#**Q4) Write a program that uses a bigram language model to predict the next word in a given context. The program should consider the previous word to predict the next word with the highest probability.**\n",
    "Task: Train a bigram model on a text corpus and predict the next word for a given input word.\n",
    "\n",
    "**Example:**\n",
    "Once upon a time there was a brave knight.\n",
    "\n",
    "The knight was known for his bravery and courage.\n",
    "\n",
    "He fought many battles and won the hearts of the people.\n",
    "\n",
    "The people of the kingdom loved the knight.\n",
    "\n",
    "One day, the knight embarked on a journey to defeat a dragon.\n",
    "\n",
    "The dragon was terrorizing the villages in the kingdom.\n",
    "\n",
    "After a long and fierce battle, the knight defeated the dragon.\n",
    "\n",
    "The people celebrated the victory of the knight.\n",
    "\n",
    "The knight became a legend in the kingdom.\n",
    "\n",
    "\n",
    "**what is most probable next word predicted by the modal?**\n",
    "\n",
    "\"The knight\"\n",
    "\n",
    "\"The people\"\n",
    "\n",
    "\"The dragon\"\n",
    "\n",
    "\"A brave\"\n",
    "\n",
    "\"The kingdom\"\n",
    "\n",
    "**Expected Predictions:**\n",
    "For \"The knight\", the model might predict \"was\".\n",
    "\n",
    "For \"The people\", it might predict \"of\" or \"celebrated\".\n",
    "\n",
    "For \"The dragon\", it could predict \"was\" or \"defeated\".\n",
    "\n",
    "For \"A brave\", it might predict \"knight\".\n",
    "\n",
    "For \"The kingdom\", it could predict \"loved\" or \"celebrated\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n0WZhH_f6U0"
   },
   "source": [
    "- I believe that the example was misplaced here as the example discusses a trigram model whereas a bigram model was used for prediction of the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4aJ3JtOcIu7",
    "outputId": "0b5a55df-f0f3-49eb-869a-586c904eefc8"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_text = \"\"\"\n",
    "Once upon a time there was a brave knight. The knight was known for his bravery and courage.\n",
    "He fought many battles and won the hearts of the people. The people of the kingdom loved the knight.\n",
    "One day, the knight embarked on a journey to defeat a dragon. The dragon was terrorizing the villages in the kingdom.\n",
    "After a long and fierce battle, the knight defeated the dragon. The people celebrated the victory of the knight.\n",
    "The knight became a legend in the kingdom.\n",
    "\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(test_text.lower())\n",
    "\n",
    "bigrams = list( ngrams(tokens, 2))\n",
    "\n",
    "freq_grams = defaultdict(Counter)\n",
    "for w1, w2 in bigrams:\n",
    "    freq_grams[w1][w2] += 1\n",
    "\n",
    "def predict_next( w1 ,freq_grams):\n",
    "    key = w1\n",
    "    if key in freq_grams:\n",
    "        options = freq_grams[key].most_common()\n",
    "\n",
    "        options = [(w, coun) for w, coun in options if w not in string.punctuation]\n",
    "        if options:\n",
    "            return options[0][0]\n",
    "        else:\n",
    "            return \"Cannot predict further word\"\n",
    "    else:\n",
    "        return \"Cannot predict further word\"\n",
    "\n",
    "\n",
    "print(predict_next(\"the\", freq_grams))\n",
    "print(predict_next(\"and\", freq_grams))\n",
    "print(predict_next(\"a\", freq_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8nNCRrScJKL"
   },
   "source": [
    "#**Q5) Write a program that implements an N-gram (e.g., trigram) language model. The program should calculate the probability of a given sequence of words.**\n",
    "Task: Implement a trigram model using a provided text corpus. Calculate the probability of a given sentence.\n",
    "\n",
    "Corpus:\n",
    "\n",
    "I am a human.\n",
    "\n",
    "I am not a stone.\n",
    "\n",
    "I I live in Mumbai.\n",
    "\n",
    "Check the probability of \" I I am not \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFj9ZtVbzTNx"
   },
   "source": [
    "- Below, I have a tri-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-1JtgxHdXS6",
    "outputId": "282a2228-3644-4ce1-999f-387a9f0471eb"
   },
   "outputs": [],
   "source": [
    "test_text2= \"\"\"\n",
    "I am a human.\n",
    "I am not a stone.\n",
    "I I live in Mumbai.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokens = nltk.word_tokenize(test_text.lower())\n",
    "\n",
    "# Generate 3-grams and 2-grams\n",
    "thregrams = list(ngrams(tokens, 3))\n",
    "twograms = list(ngrams(tokens, 2))\n",
    "\n",
    "# Count the frequencies of 4-grams and 3-grams\n",
    "freq_trigrams = defaultdict(Counter)\n",
    "freq_bigrams = Counter()\n",
    "\n",
    "for w1, w2, w3 in thregrams:\n",
    "    freq_trigrams[(w1, w2)][w3] += 1\n",
    "\n",
    "for w1, w2 in twograms:\n",
    "    freq_bigrams[(w1, w2)] += 1\n",
    "\n",
    "def find_probability(sentence, trigram_freq, bigram_freq):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    trigrams = list(ngrams(tokens, 3))\n",
    "\n",
    "    prob = 1.0\n",
    "    for w1, w2, w3 in trigrams:\n",
    "\n",
    "        bigram = (w1, w2)\n",
    "        coun_bigram = bigram_freq[bigram]\n",
    "        coun_trigram = trigram_freq[bigram][w3]\n",
    "\n",
    "        # applying laplacian smoothing\n",
    "        vocab_size = len( set(tokens) )\n",
    "        prob *= ( coun_trigram + 1 ) / ( coun_bigram + vocab_size )\n",
    "\n",
    "    return prob\n",
    "\n",
    "sen = \"I I am not\"\n",
    "probability = find_probability( sen, freq_trigrams, freq_bigrams )\n",
    "\n",
    "print(f'Probability of \"{sen}\": {probability}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVTsuDc_dXi5"
   },
   "source": [
    "#**Q6) Write a program that applies smoothing techniques (e.g., Laplace, Good-Turing) to a bigram language model. Compare the performance of the model with and without smoothing.**\n",
    "\n",
    "Corpus:\n",
    "\n",
    "I love natural language processing.\n",
    "\n",
    "Language models are great.\n",
    "\n",
    "I love machine learning.\n",
    "\n",
    "Machine learning is fun.\n",
    "\n",
    "Natural language processing is a complex field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "phFEP9aGdcMp",
    "outputId": "5bbf49ce-5a6f-4232-aece-f1fff325f823"
   },
   "outputs": [],
   "source": [
    "# Corpus\n",
    "test_text3 = \"\"\"\n",
    "I love natural language processing.\n",
    "Language models are great.\n",
    "I love machine learning.\n",
    "Machine learning is fun.\n",
    "Natural language processing is a complex field.\n",
    "\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(test_text3.lower())\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "freq_bigrams = Counter(bigrams)\n",
    "freq_onegrams = Counter(tokens)\n",
    "\n",
    "def without_smoothing( w1, w2, freq_bigrams, freq_onegram ):\n",
    "\n",
    "    bigram = ( w1, w2)\n",
    "    coun_bigram = freq_bigrams[bigram ]\n",
    "    coun_word = freq_onegram[ w1 ]\n",
    "\n",
    "    if coun_word == 0:\n",
    "        return 0\n",
    "\n",
    "    return coun_bigram / coun_word\n",
    "\n",
    "def laplace_smoothing(w1, w2, freq_bigrams, freq_onegram ):\n",
    "\n",
    "    bigram = (w1, w2)\n",
    "    n = len(set(tokens))\n",
    "    coun_bigram = freq_bigrams[ bigram ]\n",
    "    coun_word = freq_onegram[ w1 ]\n",
    "\n",
    "    return (coun_bigram + 1) / (coun_word + n)\n",
    "\n",
    "def sentence_probab(sentence, bigram_freqs, word_freqs, choice):\n",
    "\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "    prob = 1.0\n",
    "    if(choice==0):\n",
    "      for w1, w2 in bigrams:\n",
    "        prob *= without_smoothing(w1, w2, bigram_freqs, word_freqs)\n",
    "\n",
    "    elif(choice==1):\n",
    "      for w1, w2 in bigrams:\n",
    "        prob *= laplace_smoothing(w1, w2, freq_bigrams, freq_onegrams)\n",
    "\n",
    "    return prob\n",
    "\n",
    "sentence = \"I love machine processing\"\n",
    "\n",
    "unsmoothed_prob_value = sentence_probab(sentence, freq_bigrams, freq_onegrams, 0)\n",
    "print(f'Unsmoothed probability of \"{sentence}\": {unsmoothed_prob_value}')\n",
    "\n",
    "laplace_prob_value = sentence_probab(sentence, freq_bigrams, freq_onegrams, 1)\n",
    "print(f'Laplace smoothed probability of \"{sentence}\": {laplace_prob_value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJd9yLSVjGPz"
   },
   "source": [
    "#**Q7) Write a program that implements a simple rule-based POS tagger. The program should take a sentence as input and assign POS tags based on predefined rules. For instance, you might define rules such as:**\n",
    "\n",
    "Words ending in \"ed\" are tagged as verbs (e.g., \"walked\" → VBD).\n",
    "\n",
    "Words ending in \"ly\" are tagged as adverbs (e.g., \"quickly\" → RB).\n",
    "Example:\n",
    "\n",
    "Input: \"The cat quickly jumped over the lazy dog.\"\n",
    "\n",
    "Output: [('The', 'DT'), ('cat', 'NN'), ('quickly', 'RB'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDgU-ygCjSl9",
    "outputId": "9010d6ff-56dc-45d4-8e62-6f3fbb835e6c"
   },
   "outputs": [],
   "source": [
    "def Tagging_Word(word):\n",
    "\n",
    "    if word.lower() in {'an', 'the', 'a'}:\n",
    "        return 'DT'\n",
    "    elif word.lower() in {'by', 'with', 'for', 'of', 'in', 'on', 'at'}:\n",
    "        return 'IN'\n",
    "    elif word.lower() in {'is', 'am', 'are', 'was', 'were', 'shall', 'has', 'have', 'had'}:\n",
    "        return 'VB'\n",
    "    elif word in {'.', '-', '!', '?', ';', ',', ':'}:\n",
    "        return 'PUNCT'\n",
    "    elif word.endswith('ed'):\n",
    "        return 'VBD'\n",
    "    elif word.endswith('ly'):\n",
    "        return 'RB'\n",
    "    elif word.endswith('ing'):\n",
    "        return 'VBG'\n",
    "    elif word.endswith('s') and not word.endswith('ss'):\n",
    "        return 'VBZ'\n",
    "    elif word.endswith('ness') or word.endswith('ity'):\n",
    "        return 'NN'\n",
    "    elif word.endswith('er') or word.endswith('est') or ( word.endswith('y') and ( word[:-1] == \"cloud\" or word[:-1]==\"malt\" or word[:-1]==\"mess\" ) ):\n",
    "        return 'JJ'\n",
    "    elif word.endswith('ion') or word.endswith('ment'):\n",
    "        return 'NN'\n",
    "    else:\n",
    "        return 'NN'\n",
    "\n",
    "def tag_sentence(sen):\n",
    "\n",
    "    words = word_tokenize(sen)\n",
    "    tagged_words = [(word, Tagging_Word(word)) for word in words]\n",
    "\n",
    "    return tagged_words\n",
    "\n",
    "sen1 = \"The cat quickly jumped over the lazy dog.\"\n",
    "sen2= \"It was a cloudy day in the morning.\"\n",
    "\n",
    "print( tag_sentence(sen1) )\n",
    "print( tag_sentence(sen2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw6ZiwF7DkNL"
   },
   "source": [
    "The inference here is that it is very difficult to classify words into different POS as there could be many more exceptions like \"cloudy\" and \"day\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
